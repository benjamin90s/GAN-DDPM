{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # First layer - input is the latent vector Z (nz-dimension)\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),  # Dropout layer to enhance diversity\n",
    "\n",
    "            # Second layer\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.3),  # Another dropout layer\n",
    "\n",
    "            # Third layer\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Fourth layer\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # Output layer - generates the output image with nc channels (number of color channels)\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()  # Tanh activation to output values between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "    \n",
    "    \n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # output. 1 x 1 x 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "# Hyperparameters\n",
    "nz = 100  # Size of z latent vector (i.e. size of generator input)\n",
    "ngf = 64  # Size of feature maps in generator\n",
    "ndf = 64  # Size of feature maps in discriminator\n",
    "nc = 3    # Number of channels in the training images. For color images this is 3\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "# Create the generator and discriminator\n",
    "netG = Generator(nz, ngf, nc)\n",
    "netD = Discriminator(ndf, nc)\n",
    "\n",
    "# Initialize weights\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG.to(device)\n",
    "netD.to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training Data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Training the GAN\n",
    "num_epochs = 50\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)  # 64 is the number of images you want to generate\n",
    "img_list = []\n",
    "\n",
    "# Training loop corrections\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        ############################\n",
    "        # (1) Update Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # Train with real data\n",
    "        real_data = data[0].to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "        output = netD(real_data).squeeze()  # Flatten the output\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Train with fake data\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach()).squeeze()  # Flatten the output\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake  # Total discriminator loss\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update Generator: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # Fake labels are real for generator cost\n",
    "        output = netD(fake_data).squeeze()  # Flatten the output\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z1)): %.4f\\tD(G(z2)): %.4f'\n",
    "                  % (epoch, num_epochs, i, len(trainloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "    # Check progress by generating fake image after each epoch\n",
    "    with torch.no_grad():\n",
    "        fake = netG(fixed_noise).detach().cpu()\n",
    "    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Generate a fixed random vector for image generation\n",
    "fixed_noise = torch.randn(100, nz, 1, 1, device=device)  # 100 for a 10x10 grid\n",
    "\n",
    "# Generate images from the fixed noise\n",
    "with torch.no_grad():\n",
    "    netG.eval()  # Set the generator to evaluation mode\n",
    "    generated_images = netG(fixed_noise).detach().cpu()\n",
    "\n",
    "# Create a grid of images\n",
    "grid = vutils.make_grid(generated_images, nrow=10, normalize=True)\n",
    "\n",
    "# Function to display the images\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Images Grid\")\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))  # Convert from Tensor image\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "from torch import optim, nn\n",
    "\n",
    "def train_gan(training_ratios, num_epochs=50):\n",
    "    results = {}\n",
    "    fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "    for ratio in training_ratios:\n",
    "        netG = Generator(nz, ngf, nc).to(device)\n",
    "        netD = Discriminator(ndf, nc).to(device)\n",
    "        netG.apply(weights_init)\n",
    "        netD.apply(weights_init)\n",
    "\n",
    "        optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        G_losses = []\n",
    "        D_losses = []\n",
    "        image_snapshots = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                real_data = data[0].to(device)\n",
    "                batch_size = real_data.size(0)\n",
    "                real_label = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
    "                fake_label = torch.full((batch_size,), 0, dtype=torch.float, device=device)\n",
    "\n",
    "                # Train discriminator 'D' times\n",
    "                for _ in range(ratio['D']):\n",
    "                    netD.zero_grad()\n",
    "                    output_real = netD(real_data).squeeze()\n",
    "                    errD_real = criterion(output_real, real_label)\n",
    "                    errD_real.backward()\n",
    "\n",
    "                    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "                    fake_data = netG(noise)\n",
    "                    output_fake = netD(fake_data.detach()).squeeze()\n",
    "                    errD_fake = criterion(output_fake, fake_label)\n",
    "                    errD_fake.backward()\n",
    "\n",
    "                    optimizerD.step()\n",
    "\n",
    "                # Train generator 'G' times\n",
    "                for _ in range(ratio['G']):\n",
    "                    netG.zero_grad()\n",
    "                    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "                    fake_data = netG(noise)\n",
    "                    output = netD(fake_data).squeeze()\n",
    "                    errG = criterion(output, real_label)\n",
    "                    errG.backward()\n",
    "                    optimizerG.step()\n",
    "\n",
    "                # Logging\n",
    "                G_losses.append(errG.item())\n",
    "                D_losses.append((errD_real + errD_fake).item())\n",
    "\n",
    "                if i % 100 == 0 or (i == len(trainloader)-1 and epoch == num_epochs-1):\n",
    "                    print(f\"Ratio: {ratio['label']} | Epoch [{epoch+1}/{num_epochs}] | Batch [{i}/{len(trainloader)}] | \"\n",
    "                          f\"Loss_D: {(errD_real + errD_fake).item():.4f} | Loss_G: {errG.item():.4f}\")\n",
    "                    with torch.no_grad():\n",
    "                        fixed_fake = netG(fixed_noise).detach().cpu()\n",
    "                        image_snapshots.append(vutils.make_grid(fixed_fake, padding=2, normalize=True))\n",
    "\n",
    "        # Save results\n",
    "        results[ratio['label']] = {\n",
    "            'G_losses': G_losses,\n",
    "            'D_losses': D_losses,\n",
    "            'images': image_snapshots,\n",
    "            'netG': netG\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def plot_losses(results):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for key, result in results.items():\n",
    "        G_losses = result['G_losses']\n",
    "        D_losses = result['D_losses']\n",
    "        plt.plot(G_losses, label=f'G Loss {key}')\n",
    "        plt.plot(D_losses, label=f'D Loss {key}')\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define training ratios to test\n",
    "training_ratios = [\n",
    "    {'D': 1, 'G': 2, 'label': '1:2'},\n",
    "    {'D': 2, 'G': 1, 'label': '2:1'},\n",
    "    {'D': 2, 'G': 2, 'label': '2:2'}\n",
    "]\n",
    "\n",
    "\n",
    "# Run the experiment\n",
    "results = train_gan(training_ratios)\n",
    "\n",
    "# Now calling the plot_losses function to visualize the losses\n",
    "plot_losses(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images_from_ratios(results, nrow=10, image_size=(18, 18)):\n",
    "    num_configs = len(results)\n",
    "    fig, axes = plt.subplots(num_configs, 1, figsize=(image_size[0], image_size[1] * num_configs))\n",
    "\n",
    "    # Ensure axes is iterable\n",
    "    if num_configs == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (label, result) in zip(axes, results.items()):\n",
    "        image_snapshots = result['images']\n",
    "        last_image_batch = image_snapshots[-1]  # Last snapshot\n",
    "\n",
    "        grid = vutils.make_grid(last_image_batch, nrow=nrow, normalize=True)\n",
    "\n",
    "        ax.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Generated Images - Training Ratio {label}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_generated_images_from_ratios(results)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.makedirs(\"GAN_fid_images/real\", exist_ok=True)\n",
    "\n",
    "real_loader = DataLoader(trainset, batch_size=1, shuffle=True)\n",
    "for i, (img, _) in enumerate(real_loader):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    save_image(img, f\"GAN_fid_images/real/real_{i}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_images(netG, save_path, nz=100):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    netG.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(1000):\n",
    "            noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "            fake_img = netG(noise).detach().cpu()\n",
    "            save_image(fake_img, f\"{save_path}/fake_{i}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_generated_images(results['1:2']['netG'], \"GAN_fid_images/fake_1_2\")\n",
    "save_generated_images(results['2:1']['netG'], \"GAN_fid_images/fake_2_1\")\n",
    "save_generated_images(results['2:2']['netG'], \"GAN_fid_images/fake_2_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytorch_fid GAN_fid_images/real GAN_fid_images/fake_1_2\n",
    "!python -m pytorch_fid GAN_fid_images/real GAN_fid_images/fake_2_1\n",
    "!python -m pytorch_fid GAN_fid_images/real GAN_fid_images/fake_2_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_multiple_ratios(results, ratios=['1:2', '2:1', '2:2'], nz=100, steps=10):\n",
    "    fig, axes = plt.subplots(len(ratios), 1, figsize=(steps * 2, len(ratios) * 3))\n",
    "\n",
    "    if len(ratios) == 1:\n",
    "        axes = [axes]  # Ensure iterable\n",
    "\n",
    "    for ax, label in zip(axes, ratios):\n",
    "        netG = results[label]['netG']\n",
    "        netG.eval()\n",
    "\n",
    "        z_start = torch.randn(1, nz, 1, 1, device=device)\n",
    "        z_end = torch.randn(1, nz, 1, 1, device=device)\n",
    "\n",
    "        interpolated_z = [(1 - alpha) * z_start + alpha * z_end for alpha in torch.linspace(0, 1, steps)]\n",
    "        interpolated_z = torch.cat(interpolated_z, dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            interpolated_images = netG(interpolated_z).cpu()\n",
    "\n",
    "        grid = vutils.make_grid(interpolated_images, nrow=steps, normalize=True)\n",
    "        ax.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "        ax.set_title(f\"Latent Interpolation - {label}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_multiple_ratios(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the full AFHQ dataset (train split)\n",
    "dataset = load_dataset(\"huggan/AFHQ\", split=\"train\")\n",
    "\n",
    "# Define output root directory\n",
    "root_dir = \"afhq_all\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "\n",
    "# Map numerical labels to class names\n",
    "label_map = {0: 'cat', 1: 'dog', 2: 'wild'}\n",
    "\n",
    "# Iterate and save images in class-specific subfolders\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    item = dataset[i]\n",
    "    img = item['image']\n",
    "    label = label_map[item['label']]\n",
    "    \n",
    "    class_dir = os.path.join(root_dir, label)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    \n",
    "    img_path = os.path.join(class_dir, f\"afhq_{i}.jpg\")\n",
    "    img.save(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision import utils as vutils\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = np.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
    "    return emb\n",
    "\n",
    "def nonlinearity(x):\n",
    "    return x*torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "\n",
    "        # Optional convolution after upsampling\n",
    "        if self.with_conv:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels, in_channels,\n",
    "                kernel_size=3, stride=1, padding=1\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Nearest-neighbor upsampling (2x)\n",
    "        x = nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "\n",
    "        # Apply conv\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "\n",
    "        # convolution for strided downsampling\n",
    "        if self.with_conv:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels, in_channels,\n",
    "                kernel_size=3, stride=2, padding=0\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.with_conv:\n",
    "            # Pad input to maintain even spatial dimensions\n",
    "            pad = (0, 1, 0, 1)  # (left, right, top, bottom)\n",
    "            x = nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "            x = self.conv(x)\n",
    "        else:\n",
    "            # Average pooling if conv is not used\n",
    "            x = nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
    "                 dropout, temb_channels=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "\n",
    "        # First normalization and convolution\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Time embedding projection to match output channels\n",
    "        self.temb_proj = nn.Linear(temb_channels, self.out_channels)\n",
    "\n",
    "        # Second normalization and convolution with dropout\n",
    "        self.norm2 = Normalize(self.out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Shortcut projection (1x1 or 3x3 conv if needed)\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                self.conv_shortcut = nn.Conv2d(in_channels, self.out_channels, kernel_size=3, padding=1)\n",
    "            else:\n",
    "                self.nin_shortcut = nn.Conv2d(in_channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.norm1(x)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        # Add projected time embedding\n",
    "        h += self.temb_proj(nonlinearity(temb))[:, :, None, None]\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        # Apply shortcut if dimensions don't match\n",
    "        if self.in_channels != self.out_channels:\n",
    "            x = self.conv_shortcut(x) if self.use_conv_shortcut else self.nin_shortcut(x)\n",
    "\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, config, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Configuration Parameters ---\n",
    "        self.config = config\n",
    "        ch = config['ch']\n",
    "        out_ch = config['out_ch']\n",
    "        ch_mult = tuple(config['ch_mult'])\n",
    "        num_res_blocks = config['num_res_blocks']\n",
    "        attn_resolutions = config['attn_resolutions']\n",
    "        dropout = config['dropout']\n",
    "        in_channels = config['in_channels']\n",
    "        resolution = config['image_size']\n",
    "        resamp_with_conv = config['resamp_with_conv']\n",
    "\n",
    "        self.ch = ch\n",
    "        self.temb_ch = ch * 4\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # --- Time Embedding ---\n",
    "        self.temb = nn.Module()\n",
    "        self.temb.dense = nn.ModuleList([\n",
    "            nn.Linear(ch, self.temb_ch),\n",
    "            nn.Linear(self.temb_ch, self.temb_ch)\n",
    "        ])\n",
    "\n",
    "        # --- Input Convolution ---\n",
    "        self.conv_in = nn.Conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # --- Downsampling Blocks ---\n",
    "        self.down = nn.ModuleList()\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1,) + ch_mult\n",
    "\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            if verbose:\n",
    "                print(f'Downsampling - Resolution: {curr_res}')\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "\n",
    "            block_in = ch * in_ch_mult[i_level]\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "\n",
    "            for i_block in range(num_res_blocks):\n",
    "                block.append(ResnetBlock(\n",
    "                    in_channels=block_in,\n",
    "                    out_channels=block_out,\n",
    "                    temb_channels=self.temb_ch,\n",
    "                    dropout=dropout\n",
    "                ))\n",
    "                block_in = block_out\n",
    "\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(AttnBlock(block_in))\n",
    "\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "\n",
    "            self.down.append(down)\n",
    "\n",
    "        # --- Middle Blocks ---\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.mid.attn_1 = AttnBlock(block_in)\n",
    "        self.mid.block_2 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # --- Upsampling Blocks ---\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            skip_in = ch * ch_mult[i_level]\n",
    "\n",
    "            for i_block in range(num_res_blocks + 1):\n",
    "                if i_block == num_res_blocks:\n",
    "                    skip_in = ch * in_ch_mult[i_level]\n",
    "\n",
    "                block.append(ResnetBlock(\n",
    "                    in_channels=block_in + skip_in,\n",
    "                    out_channels=block_out,\n",
    "                    temb_channels=self.temb_ch,\n",
    "                    dropout=dropout\n",
    "                ))\n",
    "                block_in = block_out\n",
    "\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(AttnBlock(block_in))\n",
    "\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "\n",
    "            self.up.insert(0, up)\n",
    "\n",
    "        # --- Output Layers ---\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        assert x.shape[2] == x.shape[3] == self.resolution, \"Input resolution mismatch.\"\n",
    "\n",
    "        # --- Timestep Embedding ---\n",
    "        temb = get_timestep_embedding(t, self.ch)\n",
    "        temb = nonlinearity(self.temb.dense[0](temb))\n",
    "        temb = self.temb.dense[1](temb)\n",
    "\n",
    "        # --- Downsampling ---\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        # --- Middle ---\n",
    "        h = hs[-1]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # --- Upsampling ---\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                h = self.up[i_level].block[i_block](torch.cat([h, hs.pop()], dim=1), temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # --- Final Output ---\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        return self.conv_out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.norm = Normalize(in_channels)\n",
    "\n",
    "        # Projections for query, key, value\n",
    "        self.q = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.k = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "        self.v = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "        # Output projection\n",
    "        self.proj_out = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.norm(x)\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        q = self.q(h)\n",
    "        k = self.k(h)\n",
    "        v = self.v(h)\n",
    "\n",
    "        b, c, h, w = q.shape\n",
    "        q = q.reshape(b, c, h * w).permute(0, 2, 1)  # [B, HW, C]\n",
    "        k = k.reshape(b, c, h * w)                   # [B, C, HW]\n",
    "        v = v.reshape(b, c, h * w)                   # [B, C, HW]\n",
    "\n",
    "        # Attention weights: [B, HW, HW]\n",
    "        attn = torch.bmm(q, k) * (c ** -0.5)\n",
    "        attn = torch.softmax(attn, dim=2)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn = attn.permute(0, 2, 1)                 # [B, HW, HW]\n",
    "        h_out = torch.bmm(v, attn).reshape(b, c, h, w)\n",
    "\n",
    "        return x + self.proj_out(h_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    def __init__(self, diffusion_config, device):\n",
    "        self.device = device\n",
    "        self.eta = diffusion_config['eta'] \n",
    "        self.sampling_scheme = diffusion_config['sampling_scheme']\n",
    "        self.num_timesteps = diffusion_config['num_timesteps']\n",
    "\n",
    "        # Linear beta schedule from beta_start to beta_end\n",
    "        self.betas = torch.linspace(\n",
    "            diffusion_config['beta_start'],\n",
    "            diffusion_config['beta_end'],\n",
    "            self.num_timesteps,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Alpha and its derived forms\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1.0 - self.alpha_cum_prod)\n",
    "\n",
    "    def add_noise(self, original, noise, t):\n",
    "        \"\"\"\n",
    "        Adds noise to the original input `x_0` at timestep `t`:\n",
    "            x_t = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * noise\n",
    "        \"\"\"\n",
    "        batch_size = original.shape[0]\n",
    "\n",
    "        # Select alpha_t and (1 - alpha_t) for each sample\n",
    "        sqrt_alpha = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
    "\n",
    "        # Expand to match image shape\n",
    "        for _ in range(len(original.shape) - 1):\n",
    "            sqrt_alpha = sqrt_alpha.unsqueeze(-1)\n",
    "            sqrt_one_minus_alpha = sqrt_one_minus_alpha.unsqueeze(-1)\n",
    "\n",
    "        return sqrt_alpha * original + sqrt_one_minus_alpha * noise\n",
    "\n",
    "    def sample_prev_timestep(self, x_t, noise_pred, t_i_minus_1, t_i=None):\n",
    "        \"\"\"\n",
    "        DDPM reverse step: estimate x_{t-1} from x_t and predicted noise.\n",
    "        \"\"\"\n",
    "        # Predict x0 from x_t and predicted noise\n",
    "        x0_pred = (x_t - self.sqrt_one_minus_alpha_cum_prod[t_i] * noise_pred) / self.sqrt_alpha_cum_prod[t_i]\n",
    "        x0_pred = torch.clamp(x0_pred, -1.0, 1.0)\n",
    "\n",
    "        # Compute mean of posterior q(x_{t-1} | x_t, x0)\n",
    "        mean = x_t - (self.betas[t_i] * noise_pred) / self.sqrt_one_minus_alpha_cum_prod[t_i]\n",
    "        mean = mean / torch.sqrt(self.alphas[t_i])\n",
    "\n",
    "        if t_i_minus_1 == 0:\n",
    "            return mean, x0_pred\n",
    "\n",
    "        if self.sampling_scheme == 'DDPM':\n",
    "            # Variance for noise addition\n",
    "            var_ratio = (1.0 - self.alpha_cum_prod[t_i_minus_1]) / (1.0 - self.alpha_cum_prod[t_i])\n",
    "            variance = var_ratio * self.betas[t_i]\n",
    "            sigma = torch.sqrt(variance)\n",
    "\n",
    "            # Add Gaussian noise\n",
    "            noise = torch.randn_like(x_t).to(self.device)\n",
    "            return mean + sigma * noise, x0_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "img_size = 64\n",
    "img_channels = 3\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "lr = 3e-4\n",
    "Gen_samples = 100\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Configurations ---\n",
    "model_config = {\n",
    "    'type': \"simple\",\n",
    "    'in_channels': img_channels,\n",
    "    'out_ch': img_channels,\n",
    "    'ch': 128,\n",
    "    'ch_mult': [1, 1, 2, 2, 4],\n",
    "    'num_res_blocks': 3,\n",
    "    'attn_resolutions': [16],\n",
    "    'dropout': 0.0,\n",
    "    'var_type': 'fixedsmall',\n",
    "    'ema_rate': 0.999,\n",
    "    'ema': True,\n",
    "    'resamp_with_conv': True,\n",
    "    'image_size': img_size\n",
    "}\n",
    "\n",
    "diffusion_config = {\n",
    "    'num_timesteps': 1000,\n",
    "    'beta_start': 0.0001,\n",
    "    'beta_end': 0.02,\n",
    "    'sampling_scheme': 'DDPM',\n",
    "    'eta': 0,\n",
    "}\n",
    "\n",
    "train_config = {\n",
    "    'batch_size': batch_size,\n",
    "    'num_epochs': epochs,\n",
    "    'Gen_samples': Gen_samples,\n",
    "    'num_grid_rows': 10,\n",
    "    'lr': lr,\n",
    "    'betas': (0.9, 0.95),\n",
    "    'max_grad_norm': 1\n",
    "}\n",
    "\n",
    "# --- Data Loading ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Automatically get the current working directory and dataset path\n",
    "data_root = os.path.join(os.getcwd(), \"afhq_all\")\n",
    "dataset = ImageFolder(root=data_root, transform=transform)\n",
    "\n",
    "\n",
    "#dataset = ImageFolder(root=\"/home/adityab/ADRL/A1/Dlcv_ass/afhq_all\", transform=transform)\n",
    "val_size = 1000\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Model and Scheduler ---\n",
    "#model = Unet(model_config).to(device)\n",
    "model = torch.compile(Unet(model_config).to(device))\n",
    "scheduler = NoiseScheduler(diffusion_config, device)\n",
    "optimizer = AdamW(model.parameters(), lr=train_config['lr'], betas=train_config['betas'], weight_decay=0.002)\n",
    "\n",
    "num_warmup_steps = len(train_loader) * 2\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "opt_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sampler ---\n",
    "@torch.no_grad()\n",
    "def sampler(epoch, model, scheduler, diffusion_config, model_config, train_config, show_img=True):\n",
    "    model.eval()\n",
    "    num_samples = train_config['Gen_samples']\n",
    "    img_size = model_config['image_size']\n",
    "    channels = model_config['out_ch']\n",
    "    num_timesteps = diffusion_config['num_timesteps']\n",
    "\n",
    "    # Start from Gaussian noise\n",
    "    x = torch.randn(num_samples, channels, img_size, img_size).to(device)\n",
    "\n",
    "    # Reverse diffusion loop\n",
    "    for i in reversed(range(num_timesteps)):\n",
    "        t = torch.full((num_samples,), i, dtype=torch.long).to(device)\n",
    "        noise_pred = model(x, t)\n",
    "        x, _ = scheduler.sample_prev_timestep(x, noise_pred, t_i_minus_1=i - 1 if i > 0 else 0, t_i=i)\n",
    "\n",
    "    # Convert to [0,1] range\n",
    "    x = (x.clamp(-1, 1) + 1) / 2.0\n",
    "    grid = vutils.make_grid(x, nrow=train_config['num_grid_rows'])\n",
    "\n",
    "    if show_img:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Generated Samples (Epoch {epoch})\")\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.show()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_list = [200, 500, 1000]  # Timesteps to experiment with\n",
    "all_epoch_losses = {}             # Stores epoch-wise losses for each timestep\n",
    "\n",
    "for steps in timesteps_list:\n",
    "    print(f\"\\n====== Training with {steps} timesteps ======\\n\")\n",
    "\n",
    "    # --- Update config ---\n",
    "    diffusion_config['num_timesteps'] = steps\n",
    "\n",
    "    # --- Re-init model, scheduler, optimizer, scheduler ---\n",
    "    #model = Unet(model_config).to(device)\n",
    "    model = torch.compile(Unet(model_config).to(device))\n",
    "    scheduler = NoiseScheduler(diffusion_config, device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=train_config['lr'], betas=train_config['betas'], weight_decay=0.002)\n",
    "\n",
    "    num_warmup_steps = len(train_loader) * 2\n",
    "    num_training_steps = len(train_loader) * epochs\n",
    "    opt_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    epoch_losses = []  # Store average loss per epoch\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for images, _ in tqdm(train_loader, desc=f\"[{steps} steps] Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device) * 2 - 1  # Normalize to [-1, 1]\n",
    "\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            t = torch.randint(0, diffusion_config['num_timesteps'], (images.size(0),), device=device)\n",
    "            noisy_images = scheduler.add_noise(images, noise, t)\n",
    "\n",
    "            noise_pred = model(noisy_images, t)\n",
    "            loss = criterion(noise_pred, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            opt_scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Avg Loss: {avg_loss:.4f} | LR: {opt_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # --- Show Samples Every 20 Epochs ---\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            sampler(epoch + 1, model, scheduler, diffusion_config, model_config, train_config, show_img=True)\n",
    "\n",
    "    # Store for plotting\n",
    "    all_epoch_losses[steps] = epoch_losses\n",
    "    print(f\"\\n[âœ“] Training complete for {steps} timesteps.\\n\")\n",
    "\n",
    "# --- Plot all loss curves ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "for steps, losses in all_epoch_losses.items():\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=f\"{steps} steps\")\n",
    "\n",
    "plt.title(\"Average Training Loss Across Timesteps\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.legend(title=\"Timesteps\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Due to large cell outputs, the generated samples and logs were not fully visible in the previous cell interface(in VS code). To address this, the following additional code have been provided to extract and saved images (for DDPM 1000-step samples & loss plot) directly from the .ipynb file. (When opened in Google Colab, the full output from the previous cell is visible, but it doesn't appear in Visual Studio.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# Automatically get current working directory\n",
    "notebook_filename = \"BenjaminDebbarma3(22554).ipynb\"  # Name only, no path\n",
    "notebook_path = os.path.join(os.getcwd(), notebook_filename)\n",
    "output_dir = os.path.join(os.getcwd(), \"extracted_images_1000\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "img_count = 0\n",
    "extract_next_image = False\n",
    "\n",
    "for cell in notebook.get(\"cells\", []):\n",
    "    if cell.get(\"cell_type\") == \"code\":\n",
    "        for output in cell.get(\"outputs\", []):\n",
    "            # Check for 1000-step log in stream output\n",
    "            if output.get(\"output_type\") == \"stream\" and \"1000\" in \"\".join(output.get(\"text\", \"\")):\n",
    "                extract_next_image = True\n",
    "\n",
    "            # Extract image after matching 1000-step indicator\n",
    "            if extract_next_image and output.get(\"output_type\") == \"display_data\":\n",
    "                data = output.get(\"data\", {})\n",
    "                if \"image/png\" in data:\n",
    "                    img_data = base64.b64decode(data[\"image/png\"])\n",
    "                    out_path = os.path.join(output_dir, f\"image_{img_count:03d}.png\")\n",
    "                    with open(out_path, \"wb\") as img_file:\n",
    "                        img_file.write(img_data)\n",
    "                    img_count += 1\n",
    "                    extract_next_image = False  # reset\n",
    "\n",
    "print(f\"Extracted images associated with 1000 steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot all loss curves ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "for steps, losses in all_epoch_losses.items():\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=f\"{steps} steps\")\n",
    "\n",
    "plt.title(\"Average Training Loss Across Timesteps\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.legend(title=\"Timesteps\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "# --- Setup ---\n",
    "real_dir = \"ddpm_fid_images/real\"\n",
    "fake_dir = \"ddpm_fid_images/fake\"\n",
    "num_images = 1000  # Total images for FID\n",
    "\n",
    "os.makedirs(real_dir, exist_ok=True)\n",
    "os.makedirs(fake_dir, exist_ok=True)\n",
    "\n",
    "# --- Save real images ---\n",
    "def save_real_images(dataloader, out_dir, max_images):\n",
    "    count = 0\n",
    "    for batch, _ in tqdm(dataloader, desc=\"Saving Real Images\"):\n",
    "        for img in batch:\n",
    "            save_image(img, os.path.join(out_dir, f\"{count:05d}.png\"))\n",
    "            count += 1\n",
    "            if count >= max_images:\n",
    "                return\n",
    "\n",
    "save_real_images(val_loader, real_dir, num_images)\n",
    "\n",
    "# --- Save generated images ---\n",
    "@torch.no_grad()\n",
    "def save_generated_images(model, scheduler, out_dir, diffusion_config, model_config, max_images):\n",
    "    model.eval()\n",
    "    batch_size = 50\n",
    "    num_batches = max_images // batch_size\n",
    "    img_size = model_config['image_size']\n",
    "    channels = model_config['out_ch']\n",
    "    num_timesteps = diffusion_config['num_timesteps']\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    count = 0\n",
    "    for _ in tqdm(range(num_batches), desc=\"Saving Generated Images\"):\n",
    "        x = torch.randn(batch_size, channels, img_size, img_size).to(device)\n",
    "        for t in reversed(range(num_timesteps)):\n",
    "            t_tensor = torch.full((batch_size,), t, dtype=torch.long).to(device)\n",
    "            noise_pred = model(x, t_tensor)\n",
    "            x, _ = scheduler.sample_prev_timestep(x, noise_pred, t_i_minus_1=t-1 if t > 0 else 0, t_i=t)\n",
    "        x = (x.clamp(-1, 1) + 1) / 2.0\n",
    "        for img in x:\n",
    "            save_image(img, os.path.join(out_dir, f\"{count:05d}.png\"))\n",
    "            count += 1\n",
    "            if count >= max_images:\n",
    "                return\n",
    "\n",
    "save_generated_images(model, scheduler, fake_dir, diffusion_config, model_config, num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute FID ---\n",
    "print(\"\\nComputing FID...\")\n",
    "subprocess.run([\"python\", \"-m\", \"pytorch_fid\", real_dir, fake_dir])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
